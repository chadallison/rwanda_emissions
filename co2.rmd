---
output: github_document
knit: (function(input, ...) { rmarkdown::render(input, output_file = "README.md", envir = globalenv()) })
---

### Loading Packages

```{r message = F, warning = F, include = F}
library(tidyverse) # essential functions
library(tidymodels) # for modeling
library(tvthemes) # for custom theme
library(janitor) # cleaning data names
library(maps) # adding map to ggplot
library(caret) # NA imputation

# custom ggplot theme
theme_custom = theme_avatar() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5, size = 9, vjust = 2.5, face = "italic"),
        plot.caption = element_text(face = "italic"),
        panel.grid.major = element_line(linewidth = 0.5, colour = "#DFDAD1"),
        panel.grid.minor = element_line(linewidth = 0.5, colour = "#DFDAD1"))

# setting custom theme
theme_set(theme_custom)
```

### Data Import

```{r}
train = read_csv("data/train.csv", col_types = cols())
test = read_csv("data/test.csv", col_types = cols())
sample_submission = read_csv("data/sample_submission.csv", col_types = cols())

paste0(
  "Training data: ",
  nrow(train),
  " rows, ",
  ncol(train),
  " columns"
)
```

### Finding Missing Data

```{r}
paste0(
  "Missing training data percentage: ",
  paste0(round(sum(is.na(train)) / (nrow(train) * ncol(train)) * 100, 2), "%")
)

paste0(
  "Missing testing data percentage: ",
  paste0(round(sum(is.na(test)) / (nrow(test) * ncol(test)) * 100, 2), "%")
)
```

### EDA: Emissions by Week

```{r}
train |>
  group_by(year, week_no) |>
  summarise(emissions = mean(emission),
            .groups = "drop") |>
  arrange(year, week_no) |>
  add_rowindex() |>
  ggplot(aes(week_no, emissions)) +
  geom_line(aes(col = as.character(year)), linewidth = 1.5) +
  labs(x = "Week Number", y = "Emissions", title = "Emissions by Week", col = NULL) +
  scale_x_continuous(breaks = seq(0, 52, by = 2)) +
  scale_y_continuous(breaks = seq(0, 150, by = 5))
```

### EDA: Map of Emissions

```{r}
world_map = maps::map("world", plot = F, fill = T)
world_map_df = fortify(world_map)

train |>
  group_by(latitude, longitude) |>
  summarise(mean_emissions = mean(emission),
            .groups = "drop") |>
  ggplot() +
  geom_point(aes(x = longitude, y = latitude, col = mean_emissions, size = mean_emissions),
             show.legend = F) +
  geom_polygon(data = world_map_df, aes(x = long, y = lat, group = group),
               fill = "transparent", col = "black") +
  scale_color_gradient(low = "#84A581", high = "#E68585") +
  coord_cartesian(xlim = c(min(train$longitude), max(train$longitude)),
                  ylim = c(min(train$latitude), max(train$latitude))) +
  scale_x_continuous(breaks = seq(25, 35, by = 0.25)) +
  scale_y_continuous(breaks = seq(-4, 0, by = 0.25)) +
  labs(x = "Latitude", y = "Longitude",
       title = "Quick Map of Emissions",
       subtitle = "Larger + redder points = more emissions")
```

### EDA: Distribution of Logarithmized Emissions

```{r}
train |>
  mutate(log_emission = log(emission),
         log_emission = ifelse(log_emission == -Inf, 0, log_emission),
         year = as.character(year)) |>
  ggplot(aes(log_emission)) +
  geom_density(aes(fill = year), alpha = 0.75, col = "transparent") +
  facet_wrap(vars(year), nrow = 3, strip.position = "right") +
  labs(x = "Logarithmized Emissions", y = "Density", fill = NULL,
       title = "Distributions of Logarithmized Emissions by Year",
       subtitle = "Logarithmized to adjust for right-skew") +
  scale_x_continuous(breaks = seq(-15, 15, by = 1)) +
  theme(legend.position = "right",
        strip.text.y = element_blank())
```

### NA Imputation

```{r}
train_no_id = train |>
  select(-ID_LAT_LON_YEAR_WEEK)

imputed_df = preProcess(train_no_id, method = "bagImpute")

train2 = predict(imputed_df, newdata = train_no_id) |>
  mutate(ID_LAT_LON_YEAR_WEEK = train$ID_LAT_LON_YEAR_WEEK) |>
  select(76, 1:75)

test_no_id = test |>
  select(-ID_LAT_LON_YEAR_WEEK)

imputed_df = preProcess(test_no_id, method = "bagImpute")

test2 = predict(imputed_df, newdata = test_no_id) |>
  mutate(ID_LAT_LON_YEAR_WEEK = test$ID_LAT_LON_YEAR_WEEK) |>
  select(75, 1:74)

if (sum(is.na(train2)) == 0 & sum(is.na(test2)) == 0) {
  print("All NA values in training and testing data imputed")
} else {
  message("Imputation failed; NA values still in data")
}
```

### Building Model

```{r}
# removing ID variables
train3 = train2 |>
  select(-c(ID_LAT_LON_YEAR_WEEK, latitude, longitude, year, week_no))

# splitting into training and testing data
df_split = initial_split(
  train3,
  prop = 0.75,
  strata = emission
)

# saving training and testing data from split
df_train = training(df_split)
df_test = testing(df_split)

# data preprocessing
pre_rec = recipe(emission ~ ., data = df_train) |>
  step_corr(all_predictors(), threshold = 0.7) |>
  prep()

# creating cross-validation folds
cv_folds = bake(pre_rec, new_data = df_train) |>
  vfold_cv(v = 5)

# model specification
xgb_mod = boost_tree(
  mode = "regression",
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune()) |>
  set_engine("xgboost", objective = "reg:squarederror")

# tuning parameter specification
xgb_params = parameters(
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction()
)

# setting grid space
xgb_grid = grid_max_entropy(
  xgb_params,
  size = 25
)

# setting up workflow
xgb_wf = workflow() |>
  add_model(xgb_mod) |>
  add_formula(emission ~ .)

# tuning model
# xgb_tuned = tune_grid(
#   object = xgb_wf,
#   resamples = cv_folds,
#   grid = xgb_grid,
#   metrics = metric_set(rmse, rsq, mae),
#   control = control_grid(verbose = T)
# )

# showing best tuned model
# xgb_tuned |>
#   show_best(metric = "rmse")

# pre-tuned model and these were best metrics - tuning took around 9 hours
xgb_best_params = data.frame(
  min_n = 33,
  tree_depth = 15,
  learn_rate = 0.003650263,
  loss_reduction = 1.65734e-10
)

# finalizing model
xgb_final = xgb_mod |>
  finalize_model(xgb_best_params)
```

### Evaluating Model

```{r}
train_processed = bake(pre_rec, new_data = df_train)

train_prediction = xgb_final |>
  fit(
    formula = emission ~ .,
    data = train_processed
  ) |>
  predict(new_data = train_processed) |>
  bind_cols(df_train)

xgb_score_train = train_prediction |>
  metrics(emission, .pred) |>
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))

test_processed = bake(pre_rec, new_data = df_test)

test_prediction = xgb_final |>
  fit(
    formula = emission ~ .,
    data = test_processed
  ) |>
  predict(new_data = test_processed) |>
  bind_cols(df_test)

xgb_score_test = test_prediction |>
  metrics(emission, .pred) |>
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))

xgb_score_train |>
  mutate(set = "train") |>
  rbind(xgb_score_test |>
  mutate(set = "test")) |>
  select(metric = .metric, estimate = .estimate, set) |>
  pivot_wider(id_cols = set, names_from = "metric", values_from = "estimate")
```

### Making Final Predictions

```{r}
final_predictions = xgb_final |>
  fit(
    formula = emission ~ .,
    data = bake(pre_rec, new_data = train3)
  ) |>
  predict(new_data = test2)

final_sub_data = cbind(test2, final_predictions) |>
  select(ID_LAT_LON_YEAR_WEEK, emission = .pred)

write_csv(final_sub_data, "data/final_submission.csv")
```


































